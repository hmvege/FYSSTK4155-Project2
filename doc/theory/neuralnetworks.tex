Among the many methods developed for machine learning, neural networks, and especially deep neural networks, are among the most popular. Neural networks were suggested already in 1943 \cite{McCulloch1943} and have had many renaissances since. Currently we are experiencing such a renaissance, but in contrast to earlier periods of resurfaced interest, we now have the computer power to use neural nets efficiently.

A neural net bases itself loosely upon the biological model of neurons communicating together in the brain. A neuron cell contains most of what a normal cell contains, but it also has a long tail called an axon and some antenna like extension called dendrites. The axon of one cell can extend quite far and attach to some of the dendrites of another neural cell. Thus, the biological neural net consists of neural cells receiving input through their dendrites from many other cells and sending output through one output \citep[p. 257]{Geron}.

The computed neural networks works in a similar way. We construct "neurons" or "nodes" which are ordered in different layers where each neuron in one layer is connected to all neurons in the next layer. Initially, we start with an input layer which we feed information. Following this initial layer we have one or many hidden layers before we reach the output layer. A neural network with two or more hidden layers are called deep neural networks \citep[p. 263]{Geron}. Each neuron contains an activation function which determines the strength of the output. In the early days, a step function was used as the activation function. However, one has found that the use of a activation function with a gradient, such as the logistic function used in logistic regression, gives a better neural net. This is due to the fact that we now can apply gradient descent when optimizing the neural net which is discussed below.

To activate a neuron, it needs an input. This input is provided by all the neurons in previous layers through "wires" connecting the neurons (think of the axon to a dendrite). Each of these "wires" is weighted and all connections between one layer and the next is affected by a bias term. Thus, the output of a neuron is given as \citep[p. 260]{Geron}
\begin{equation}
    a = \sigma(z)
    \label{eqT:activation}
\end{equation}
with
\begin{align}
    z = \bm{w}^T \bm{x} + b
\end{align}
where $a$ is the output, $\bm{w}$ are the weights, $\bm{x}$ are the inputs and $b$ is the bias term. Note that if we had used a step function instead of the logit(sigmoid) function in the equation above, the neuron would either give an output of 1 or nothing, i.e. 0. When we instead use the sigmoid function, the output can be in the range of 0 to 1.

Once all neurons have been calculated in a layer, we can move on to the next and continue until we reach our output layer. Each layer can have as many neurons as the user wants. Optimizing the number of neurons in each layer is an art and requires both experience and a bit of luck. The output layer needs one neuron for each class we wish to identify.

After initial calculation of the outer layer, you will most likely have an answer that is completely rubbish. It is clear that we have to optimize the neural net. As each neurons activation function is calculated using equation \eqref{eqT:activation}, we see that we can optimize the weights between the neurons and the bias between the layers. This is done through a method called backwards propagation where one uses the cost function to identify the magnitude of the error (the cost) of a neural net and then one goes backwards through the neural net to update the weights and biases.

The backpropegation is then summed up as following(heavily influenced by the work of \citet{Nielsen}),
\begin{itemize}
    \item Compute the output error bmtor for the final layer (L) given by
    \begin{equation*}
        \delta^L = \nabla_a \mathcal{C} \odot \sigma^{'}(\bm{w}^T \cdot \bm{x} + b)
    \end{equation*}
    where $\mathcal{C}$ is the cost function and $\nabla_a$ is a bmtor who has components that are the partial derivatives $\frac{\partial \mathcal{C}}{\partial a_j^L}$ where $a_j$ is the j'th output found by using equation \eqref{eqT:activation} for $a$.
    \item Go back through all the previous layers l = L-1, L-2, ..., 2 and compute
    \begin{equation*}
        \delta^l ((\bm{w}^{l + 1})^T \delta^{l + 1}) \odot \sigma^{'}(\bm{w}^T \cdot \bm{x} + b)
    \end{equation*}
    this is where we back propagate the error.
    \item Finally, find the gradient of the cost function for the two parameters we want to change, $\bm{w}$ and $b$ by:
    \begin{align*}
        \frac{\partial \mathcal{C}}{\partial \bm{w}_{jk}^l} &= a_k^{l-1}\delta_{j}^{l} \\
        \frac{\partial \mathcal{C}}{\partial b_{j}^l} &= \delta_{j}^{l}
    \end{align*}
    where $k$ indicates the column of $\bm{w}^l$ as $j$ indicates the row.
\end{itemize}
Once all layers have been adjusted through back propagation, one can run through the whole network again and repeat the process.

In order to understand parts of the back propagation algorithm for $L$ layers and a general layer activation function $a^L(z)$, can take a quick look at how to proceed in deriving it. We start with some cost function $\CO(w^L, y)$, where $w^L$ is the output layer weights and $y$ is the \textit{true} values which we want to approach. In order to move towards optimal weights(and biases), we can perform a gradient descent by subtracting the gradient of the cost function w.r.t. the weights and biases. We begin by finding the gradient of the $\CO$ for $W^L$. Using the chain rule we get,
\begin{align}
    \frac{\partial \mathcal{C}}{\partial w^L} &= \frac{\partial z^L}{\partial w^L} \frac{\partial a^L}{\partial z^L} \frac{\partial \CO}{\partial a^L}
    \label{eq:mlp-gradient-chain}
\end{align}
From this we simply have to find each of the partial derivatives. Without specifying the cost function and the activation layer, we can only state that the first partial derivative is given as,
\begin{align*}
\frac{\partial z^L}{\partial w^L} = a^{L-1}
\end{align*}
From this, we quickly see that we need to properly defined the cost function in relation to the $\partial^L$ as we encountered earlier.

\subsubsection{Mean square error(MSE) cost function}
A common cost function may be the quadratic loss function(MSE) given by.
\begin{equation}
    \mathcal{C}_\mathrm{MSE} = \frac{1}{2N} \sum\limits_{j} (y_j - a_j^L)^2
\end{equation}
where $N$ are the total numbers of outputs in $j$ and $y$ are the true answers. For a single training sample, this becomes,
\begin{align}
    \mathcal{C}_\mathrm{MSE} = \frac{1}{2} (a^L - y)^2
    \label{eq:mse-mlp-cost}
\end{align}
We can then insert this expression into the cost chain rule expression \eqref{eq:mlp-gradient-chain}, and we get that the partial derivative
\begin{align*}
    \frac{\partial \mathcal{C}_\mathrm{CE}}{\partial w^L} &= a^{L-1}(a^L - y)\sigma'(z^L) \\
    &= a^{L-1} \delta^L
\end{align*}
If we now take the derivative for layer $L-1$, we get
\begin{align*}
    \frac{\partial \mathcal{C}_\mathrm{CE}}{\partial w^{L-1}} &= \frac{\partial a^{L-1}}{\partial w^{L-1}} \delta^L \\
    &= \frac{\partial z^{L-1}}{\partial w^{L-1}} \frac{\partial a^{L-1}}{\partial z^{L-1}} \delta^L \\
    &= a^{L-2}\sigma'(z^{L-1})\delta^L
\end{align*}

\subsubsection{Cross entropy(CE) cost function}
The CE cost function is given as
\begin{align}
    \mathcal{C}_\mathrm{CE} = - \sum_{i=1} y_i \log a^L
    \label{eq:ce-mlp-cost}
\end{align}
with its derivative as 
\begin{align}
    \frac{\partial \mathcal{C}_\mathrm{CE}}{\partial a^L} = - \sum_{i=1} \frac{y_i} {a^L}
    \label{eq:ce-mlp-cost-der}
\end{align}
If we use softmax as the layer output activation function,
\begin{align}
    z_i = \frac{\exp{z_i}}{\sum_k \exp(z_k) }
    \label{eq:softmax}
\end{align}
with $i,k$ being output classes and its derivative,
\begin{align}
    z_i = z_i(\delta_{ij} - z_j),
\end{align}
we can write the initial gradient $\delta^L$ as
\begin{align}
    \delta^L = y - a^L
\end{align}

\subsubsection{Activation layers} \label{sec:layer-acts}
The hidden layer activations can greatly affect the outcome of the neural network. We will focus on four different hidden layer activations, the first being \textbf{sigmoidal activation}.
\begin{align}
    \sigma (z) = \frac{1}{1 + \exp (-z)}
    \label{eq:act-sig}
\end{align}
with its derivative
\begin{align}
    \sigma'(z) = \sigma(z)(1 - \sigma(z))
    \label{eq:act-sig-der}
\end{align}
The \textbf{hyperbolic tangens} activation function is given as
\begin{align}
    \sigma_\mathrm{tanh}(z) = \tanh(z)
    \label{eq:act-tanh}
\end{align}
with its derivative
\begin{align}
    \sigma'_\mathrm{tanh}(z) = 1 - \tanh^2(z)
    \label{eq:act-tanh-der}
\end{align}
The \textbf{relu} or rectifier activation is given as,
\begin{align}
    \sigma_\mathrm{relu}(z) = 
    \begin{cases}
        z & \text{if } z \geq 0 \\
        0 & \text{if } z < 0 \\
    \end{cases}
    \label{eq:act-relu}
\end{align}
with its derivative
\begin{align}
    \sigma'_\mathrm{relu}(z) = 
    \begin{cases}
        1 & \text{if } z \geq 0 \\
        0 & \text{if } z < 0 \\
    \end{cases}
    \label{eq:act-relu-der}
\end{align}
The \textbf{Heaviside} activation function is given as
\begin{align}
    \sigma_\mathrm{Heaviside}(z) = 
    \begin{cases}
        1 & \text{if } z \geq 0 \\
        0 & \text{if } z < 0 \\
    \end{cases}
    \label{eq:act-heaviside}
\end{align}
with its derivative
\begin{align}
    \sigma'_\mathrm{Heaviside}(z) = 0
    \label{eq:act-heaviside-der}
\end{align}

\subsubsection{Learning rate}
When updating the weights and biases with SGD(Stochastic gradient descent), we did so by a learning rate parameter $\eta$. There are several way to define $\eta$, with the simplest one having $\eta=\mathrm{constant}$. Another option is one that is inversely decreasing as a function of the epochs. That is, for a given epoch $i_e$ out a total $N_\mathrm{epochs}$, we set the learning rate as
\begin{align}
    \eta(t_e) = \eta_0 (1 - \frac{i_e}{1 + N_\mathrm{epochs}})
    \label{eq:inverse-eta}
\end{align}
This will force the step size to decrease toward 0 as we close in on the maximum number of epochs $N_\mathrm{epochs}$.

\subsubsection{Weight initialization} \label{sec:nn-weights}
When initializing weights and biases, we will look at two ways of how this can be done. The first is through a gaussian distribution, $\mathcal(0, 1)$ which we will call \textit{large}, as the biases will have large, spread-out distribution.

Then, we will use a gaussian distribution but divided by the number of training samples, $\mathcal(0, 1/{N_\mathrm{train}})$, dubbing that one to be called \textit{default}, as this is the one we will by default use in our neural network.

The effect of these two is essentially shrinking in the initial search space, and we should expect them to converge at large epoch times.

\subsubsection{Measuring the performance}
The performance of a neural network(or any classifier), can in its simplest form be measured by the accuracy, which is defined as
\begin{align}
    \mathrm{Accuracy} = \frac{\sum^{n}_{i=1}I(t_i = y_i)}{n},
    \label{eq:mlp-accuracy}
\end{align}
where $n$ is the number of samples we are testing against, $I$ is the indicater function, which returns 1 if the prediction $t_i$ equals the true values $y_i$.