\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumerate}

% For proper referencing in article
\usepackage{hyperref}
\usepackage{url}

% For figures and graphics'n stuff
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
% \usepackage{tabularx}
\usepackage{float}

% For proper appendices
\usepackage[toc,page]{appendix}

% Algorithm packages
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

% For bold math symbols
\usepackage{bm}
\usepackage{xcolor}

% For customized hlines in tables
\usepackage{ctable}

% For having latex symbols in section titles
\usepackage{epstopdf}

% For proper citations
% \usepackage[round, authoryear]{natbib}
\usepackage[numbers]{natbib} 

% For fixing large table height
\usepackage{a4wide}

% Remembrance and checking
\newcommand{\husk}[1]{\color{red} #1 \color{black}}
\newcommand{\sjekk}[1]{\color{violet} #1 \color{black}}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\CO}{\mathcal{C}}


% \title{FYS-STK4155: Project 2}
\title{Exploring the hyperspace of Machine Learning parameters}
\author{Eirik Ramsli Hauge, Joakim Kalsnes, Hans Mathias Mamen Vege}
\date{\today}

\begin{document}
\section{Conclusion}
We found that although both linear and logistic regression can be used, neural nets that are correctly tuned will give better scores. From our experiment, this is evident from the increase in R2 and accuracy score when comparing linear and logistic regression to their neural net counterparts respectively. For our linear regression, we found similar results to previous experiments and our results for logistic regression were different from those of Metha et al.. The reason for this difference is left to future experiments to decipher, but as it stands now, the implemented logistic regression was the superior method. For our neural net, the optimal parameters were a learning rate of $\eta = 1.0e-02$, $\lambda = 1.0e-02$, 20 neurons, 500 epochs and a mini batch size of 30. It was also evident that a neural net with $\tanh$ as activation function was the optimal neural net in our case.
\end{document}