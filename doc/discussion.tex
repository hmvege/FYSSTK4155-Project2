% \documentclass[11pt]{article}

% \usepackage[utf8]{inputenc}
% \usepackage{mathtools}
% \usepackage{amsmath}
% \usepackage{amsfonts}
% \usepackage{enumerate}

% % For proper referencing in article
% \usepackage{hyperref}
% \usepackage{url}

% % For figures and graphics'n stuff
% \usepackage{graphicx}
% \usepackage{caption}
% \usepackage{subcaption}
% % \usepackage{tabularx}
% \usepackage{float}

% % For proper appendices
% \usepackage[toc,page]{appendix}

% % Algorithm packages
% \usepackage{algorithm}
% \usepackage{algorithmicx}
% \usepackage{algpseudocode}

% % For bold math symbols
% \usepackage{bm}
% \usepackage{xcolor}

% % For customized hlines in tables
% \usepackage{ctable}

% % For having latex symbols in section titles
% \usepackage{epstopdf}

% % For proper citations
% % \usepackage[round, authoryear]{natbib}
% \usepackage[numbers]{natbib} 

% % For fixing large table height
% \usepackage{a4wide}

% % Remembrance and checking
% \newcommand{\husk}[1]{\color{red} #1 \color{black}}
% \newcommand{\sjekk}[1]{\color{violet} #1 \color{black}}

% \DeclareMathOperator{\sign}{sign}
% \DeclareMathOperator*{\argmin}{argmin}
% \DeclareMathOperator*{\CO}{\mathcal{C}}


% % \title{FYS-STK4155: Project 2}
% \title{Exploring the hyperspace of Machine Learning parameters}
% \author{Eirik Ramsli Hauge, Joakim Kalsnes, Hans Mathias Mamen Vege}
% \date{\today}

% \begin{document}
\section{Discussion}

\subsection{1D Ising model}
\subsubsection{Fitting with linear regression}
By examining the results from figure \ref{fig:bias-var-franke} illustrates the effect of different $\lambda$-values. The diagonal line illustrates the particle we are looking at and the pixels to its immediate left and right are the nearest neighbours. It is clear from this figure that even though we assumed that all particles interact with each other, only the neighbouring particles will have a profound effect on each other. For Lasso, it seems that the optimal $\lambda$ is either $10^{-3}$ or $10^{-1}$. This is agreeable with the results of Metha et al \cite{2018arXiv180308823M} who found $10^{-2}$ to be the optimal parameter. For Ridge and OLS we can not seem to see any big difference between the different $\lambda$'s. The inability to find the best $\lambda$ for OLS and Ridge and the fact that we do not have a clear optimal parameter for Lasso indicates that we should have gathered more data. \\ \\
However, by looking at figure \ref{fig:linreg-r2} we can see that there is little change in the R2-score for different $\lambda$-values, indicating that it would not matter if we generated many more plots. They would most likely stay the same. It is also clear that Lasso is the most sensitive of the models, loosing ground to the other regression methods already at $\lambda = 10^{-1}$. The R2-score for Ridge starts to decline at about $\lambda = 10^{3}$, while OLS seems to maintain its quality over all $\lambda$'s. \\ \\
This trend is further confirmed by figure \ref{fig:fig:linreg-bias-variance-decomp-ridge} and \ref{fig:linreg-bias-variance-decomp-lasso} where we have used Bootstrap and $k$-fold validation to verify our results. As we can see, the variance stays about the same for all $\lambda$-values, but MSE and therefore, the bias increases. This shift in bias also happens around $\lambda = 10^{-1}$ for Lasso and $\lambda = 10^{3}$ for Ridge as the R2-score did as well.
\subsubsection{Fitting with a neural network}
Repeating the calculations using a neural net with zero hidden layers and the identity function as the activation function gives us the opportunity to repeat the above calculations using a neural net instead. As we can see from figure \ref{fig:mlp-coefs}, the difference in $N_{\text{train}}$ had little effect, but the change in $\lambda$ was significant. The change between OLS, Ridge and Lasso also represent a change in regularisation from no regularisation to L$^2$ to L$^1$ respectively. \\
In contrast to the linear regression case, there is a difference between $\lambda = 10^{-3}$ and $\lambda = 10^{-1}$. The diagonal lines once removed which signifies the nearest neighbours are about as strong for both parameters, however, there is a noticeable decrease in noise for the $\lambda = 10^{-1}$ case. Furthermore, we must note that the use of the parameter $\lambda = 10^{1}$ is more devastating for Ridge than Lasso this time around. Lasso looses a lot of information, but for Ridge the significant of nearest neighbours disappears completely. The same trend is once again evident for the R2-score plotted in figure \ref{fig:mlp-r2}. However, with this representation we can see that increasing $N_{\text{train}}$ decreases the difference between the test and train data. This is as expected.
\subsection{2D Ising model}
\subsubsection{Classification through logistic regression}
From figure \ref{fig:logreg-accuracy-sklearn-comparison} we can compare the accuracy of our implemented logistic regression method to that of SKLearn. Furthermore, we can compare our results to those of Metha et al. \cite{2018arXiv180308823M}. What is peculiar in our case is that SKlearn with Stochastic Gradient Descent (SGD) is very varied over the different $\lambda$-values, while Metha et al. have quite stable values. Both standard SKlearn and the implemented methods have stable accuracies, with the implemented train being the clear winner. A reason for this unexpected variation in accuracy for SGD might be that we perform the analyses on less data than those of Metha et al. causing our SGD to jump more randomly back and forth for higher $\lambda$-values. If the experiment was to be repeated we would double check that the SKlearn with SGD was correctly used and give it more data. Our implemented data have performed better than those of Metha et al., which is positive. However, this might also indicate that we have implemented something wrong and are just lucky. If this experiment were to be repeated we would also double check our implementation and run it over more data to truly test if we are just lucky or if our method is actually better.
\subsubsection{Classification through neural networks}
% \end{document}